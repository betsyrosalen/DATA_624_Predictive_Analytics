---
title: "DATA624 HW4"
subtitle: "Data Pre-processing"
author: "Betsy Rosalen"
date: "3/1/2020"
output: 
    html_document: 
      code_folding: hide
      css: ./style.css
      df_print: kable
      fig_caption: yes
      fig_width: 12
      fig_height: 12
      highlight: tango
      toc: yes
      toc_float:
        collapsed: no
        smooth_scroll: no
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)
library(tidyr)
library(dplyr)
library(ggplot2)
library(GGally)
library(ggcorrplot)
library(DataExplorer)
library(kableExtra)
library(AppliedPredictiveModeling)
library(caret)
library(corrplot)
library(e1071)
library(lattice)
```

## Exercise 3.1

3.1. The UC Irvine Machine Learning Repository^[http://archive.ics.uci.edu/ml/index.html] contains a data set related to glass identification. The data consist of 214 glass samples labeled as one of seven class categories. There are nine predictors, including the refractive index and percentages of eight elements: Na, Mg, Al, Si, K, Ca, Ba, and Fe.

The data can be accessed via:

```{r}
library(mlbench)
data(Glass)
str(Glass)
```

#### (a) Using visualizations, explore the predictor variables to understand their distributions as well as the relationships between predictors.

```{r}
kable(summary(Glass[,1:5]))%>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))
kable(summary(Glass[,6:10]))%>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))
```

There 9 numerical predictors (`RI`, `Na`, `Mg`, `Al`, `Si`, `K`, `Ca`, `Ba`, and `Fe`) and one categorical target with 6 levels (`Type`).  The data set is highly skewed toward categories 1 and 2 with 146 out of 214 observations falling into those two categories alone.   `Ba`, and `Fe` have their first quartile value at 0 indicating that they have a lot of zeros in their distributions.


```{r fig.height=7}
Glass[,1:9] %>%
  gather() %>% 
  ggplot(aes(value)) +
    facet_wrap(~ key, scales = "free") +
    geom_histogram(fill = '#4575b4') +
  theme(panel.background = element_blank(), legend.position="top")
```

Histograms of the predictors show that all of them exceept maybe one, `Si` are moderately to highly skewed.  The histogram below has been colored by the target categories and shows that the distributions for different targets values are slightly different (especially for `Al` and `Na`) indicating that those variables may be good predictors.  

```{r fig.height=7}
Glass %>%
  gather(-Type, key = "var", value = "val") %>%
  ggplot(aes(x = val, fill=Type)) +
  geom_histogram(bins=10, alpha=1) +
  facet_wrap(~ var, scales = "free") +
  scale_fill_manual("target",
                    values = c('#d73027','#fc8d59','#fee090','#e0f3f8','#91bfdb','#4575b4')) +
  xlab("") +
  ylab("") +
  theme(panel.background = element_blank(), legend.position="top")
```

Those differences in the distrubtions of the predictors when sepaarated by the target variable are even more evident in the bar plots below.  All of the predictors shoow a lot of variation in their distributions for each target value.  We can also see a lot of outliers which are indicated by the red dots to the top and bottom of the plot 'whiskers'.

```{r fig.height=12}
Glass %>%
  gather(-Type,key = "var", value = "val") %>%
  ggplot(aes(x=factor(Type), y=val)) +
  geom_boxplot(width=.5, fill="#58BFFF", outlier.colour="red", outlier.size = 1) +
  stat_summary(aes(colour="mean"), fun.y=mean, geom="point",
               size=2, show.legend=TRUE) +
  stat_summary(aes(colour="median"), fun.y=median, geom="point",
               size=2, show.legend=TRUE) +
  facet_wrap(~ var, scales = "free", ncol=3) +
  labs(colour="Statistics", x="", y="") +
  scale_colour_manual(values=c("#9900FF", "#3300FF")) +
  theme(panel.background=element_blank())
```

Only `RI` and `Ca` seem to be very highly correlated with each other with a correlation of about 0.81.

```{r eval=FALSE, include=FALSE}
Glass[,1:9] %>% 
  ggpairs(alpha=0.6, fill = '#4575b4') +
  theme(panel.background=element_blank(), legend.position="top",
        axis.text.x = element_text(angle=-40, vjust=1, hjust=0))
```

```{r fig.height=6, fig.width=6}
corrs <- round(cor(Glass[,1:9]),3)
ggcorrplot::ggcorrplot(corrs,
       type = 'lower', lab=T, lab_size=2)
```


```{r}
Glass %>% 
  ggscatmat(color="Type", alpha=0.6) +
  scale_color_manual(values=c('#d73027','#fc8d59','#fee090','#e0f3f8','#91bfdb','#4575b4')) +
  theme(panel.background=element_blank(), legend.position="top",
        axis.text.x = element_text(angle=-40, vjust=1, hjust=0))
```

There are no missing values in the dataset.

```{r eval=FALSE, include=FALSE}
plot_missing(Glass) 
```

```{r}
missing <- data.frame(t(apply(is.na(Glass), 2, sum)))
# kable(missing, col.names = "NA's")
row.names(missing) <- c("NA's")
kable(missing)
```


#### (b) Do there appear to be any outliers in the data? Are any predictors skewed?

```{r}

```

#### (c) Are there any relevant transformations of one or more predictors that might improve the classification model?

```{r}

```

## Exercise 3.2

The soybean data can also be found at the UC Irvine Machine Learning Repository. Data were collected to predict disease in 683 soybeans. The 35 predictors are mostly categorical and include information on the environmental conditions (e.g., temperature, precipitation) and plant conditions (e.g., left spots, mold growth). The outcome labels consist of 19 distinct classes.

The data can be loaded via:

```{r}
# library(mlbench)
data(Soybean)
## See ?Soybean for details
```

#### (a) Investigate the frequency distributions for the categorical predictors. Are any of the distributions degenerate in the ways discussed earlier in this chapter?

```{r}

```

#### (b) Roughly 18 % of the data are missing. Are there particular predictors that are more likely to be missing? Is the pattern of missing data related to the classes?

```{r}

```

#### (c) Develop a strategy for handling missing data, either by eliminating predictors or imputation.
 
```{r}

```

## Footnotes
