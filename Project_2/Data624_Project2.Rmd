---
title: "Data 624 Project 2"
author: "Zach Herold, Anthony Pagan, Betsy Rosalen"
date: "5/10/2020"
output: 
    pdf_document:
        df_print: kable
        fig_caption: yes
        fig_width: 7
        fig_height: 7
        highlight: tango
    html_document:
        df_print: kable
        fig_caption: yes
        fig_width: 6
        fig_height: 6
        highlight: tango
        toc: yes
        toc_depth: 3
        toc_float:
            collapsed: no
            smooth_scroll: no
        css: style2.css
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
options(scipen=999, digits = 2)

library(AppliedPredictiveModeling)
library(caret)
library(e1071)
library(earth)
library(faraway)
library(fpp2)
library(ggplot2)
library(gridExtra)
library(kableExtra)
require(knitr)
library(leaps)
library(lubridate)
library(MASS)
library(mlbench)
library(naniar)
library(pander)
library(pROC)
library(psych)
library(readxl)
library(reshape)
library(reshape2)
library(tidyverse)
library(tseries)
library(urca)
library(ZIM)

# Table formatting functions
kab_tab <- function(df, cap){
  df %>% kable(caption=cap) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), 
                full_width = T)
}
kab_tab2 <- function(df, cap){
  df %>% kable(caption=cap) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), 
                full_width = F)
}
# For pdf output
# kab_tab <- function(df, cap){
#     kable(df, caption=cap, "latex", booktabs = T) %>%
#         kable_styling(latex_options = c("striped", "HOLD_position", "scale_down"))
# }
```

# Project Description 

The data science team at ABC Beverage has been asked to provide an analysis of our manufacturing process, the predictive factors, and a predictive model of PH in order to comply with new regulations.  This report details the steps taken in our analysis, including the assumptions made, the methodology used, the models tested, the model selected and the selection process, and the findings and conclusions reached from our analysis.

# Data Description

We were given a dataset that consisted of 31 numerical predictor variables detailing a wide range of production processes, 1 categorical variable `Brand.Code`, and our target variable, `PH`.  Summary statistics for these variables can be seen in the two tables below.

```{r}
#Import Data
stEval<- read.csv("https://raw.githubusercontent.com/apag101/data624Group5/master/Project2/StudentEvaluation.csv?token=AB3M6K7RXH65YIAKWZBQ6KK6XA7R2", header = TRUE)
stDatao<- read.csv("https://raw.githubusercontent.com/apag101/data624Group5/master/Project2/StudentData.csv?token=AB3M6K4AYHCT5HBFJYHCCSC6XBACQ", header = TRUE)
```

```{r}
# move PH to the first column of the dataframe
stData <- stDatao[,c(26,1:25,27:33)]
stEval <- stEval[,c(26,1:25,27:33)]
```

```{r}
# https://stackoverflow.com/questions/5863097/selecting-only-numeric-columns-from-a-data-frame

num_sum <- describe(Filter(is.numeric, stData))
kab_tab(num_sum[,c(2,3,4,8,5,9,10:13)], cap="Summary statistics for numerical variables")

cat_sum <- summary(Filter(is.factor, stData))
kab_tab(cat_sum, cap="Summary of categorical variable, Brand.Code")
```

## Distributions

Our predictors have a wide range of distributions with some normal, some skewed, some bi-modal, and some with high zero inflation.  Standardization and normalization were used for model building, the specifics of which will be described for each model in the "Models" section of the report below.  Our target, `PH`, has a mostly normal distribution.

```{r fig.height=7, fig.width=9}
stData[,-2] %>%
  gather() %>% 
  ggplot(aes(value)) +
    facet_wrap(~ key, scales = "free") +
    geom_histogram(fill = '#4575b4') +
  theme(panel.background = element_blank(), legend.position="top")
```

Since we only have one categorical variable, `Brand.Code`, which is related to brand and marketing rather than manufacturing process, we plotted it against each predictor and the target to see if there were any noticeable patterns that may be relevant to our model.  As you can see in the plots below the brand code is evenly distributed among most predictors and most importantly evenly distributed in our target variable, `PH`, so it does not appear to have any predictive value and so was removed from our dataset for model training.  

```{r fig.height=7, fig.width=9}
stData %>%
  gather(-Brand.Code, key = "var", value = "val") %>%
  ggplot(aes(x = val, fill=Brand.Code)) +
  geom_histogram(bins=10, alpha=1) +
  facet_wrap(~ var, scales = "free") +
  scale_fill_manual("Brand.Code",
                    values = c('#d73027','#fc8d59','#fee090',
                               '#e0f3f8','#91bfdb','#4575b4')) +
  xlab("") +
  ylab("") +
  theme(panel.background = element_blank(), legend.position="top")
```

```{r eval=FALSE}
stData <- stData[-2]
```

## Missing Values

There was some missing data in our predictors most noticeably in `MFR`, which had 8.25% missing values as can be seen in the plot below.  There doesn't seem to be any pattern in the missingness however, so it is unlikely that it has any predictive value.  

```{r fig.width=10}
# Missing Data
vis_miss(stData[-2])
```

The plot below confirms that there is no apparent pattern in the missing values.  Incomplete cases comprise only about 17% of the substantial dataset, and since there did not seem to be any patterns to the missingness, the decision was made to remove them completely from our dataset leaving us with the remaining 2129 complete cases.

```{r fig.height=7, fig.width=7}
gg_miss_upset(stData,
              nsets = 12,
              nintersects = 18)
```

```{r eval=FALSE}
summary(complete.cases(stData))
```


```{r}
cstData <- subset(stData[-2], complete.cases(stData))
```

## Relationships Between Variables

The plots below were used to assess if there were any clear linear relationships between the predictors and the target, `PH`.  Few, if any, relationships are immediately apparent.

```{r fig.height=7, fig.width=9}
#plot checks
featurePlot(cstData[-1],cstData$PH)
```

A correlation plot shows some strong correlations between predictors.  The `findCorrelation` function from the `caret` library recommends removing the `MFR`, `Hyd.Pressure2`, `Carb.Rel`, `Air.Pressurer`, `Carb.Flow`, `Hyd.Pressure4`, and `Filler.Level` variables at a 0.85 correlation cutoff.  Upping the cutoff to 0.9 only removes one variable, `Carb.Flow`, from that list.  

```{r fig.width=8}
#Correlation Matrix
library(corrplot)
cor.plt <- cor(cstData, use = "pairwise.complete.obs", method = "pearson")
col <- colorRampPalette(c("#BB4444", "#EE9988", "#FFFFFF", "#77AADD", "#4477AA"))
corrplot(cor.plt, method="color", col=col(200),  
         type="upper", order="hclust",
         tl.col="black", tl.srt=45, tl.cex=0.5,
         diag=FALSE 
         )
```

```{r}
kab_tab(names(cstData)[findCorrelation(cor(cstData[-1]), cutoff = .85)], 
        cap = "Variables recommended for removal by caret::findCorrelation function at the 0.85 cutoff")

# sb<-names(cstData)[findCorrelation(cor(cstData[-1]), cutoff = .85)]
# cstData<-cstData[, -which(names(cstData) %in% c(sb))]
```

Before we removed any variables we decided to do our own analysis of the highly correlated variables to see if we came up with the same conclusions as the `findCorrelation` function. First we found all pairs of variables that had a 0.85 correlation or more, which was 13 pairs.  Then we found the frequency of each variable that was a member of one or more of these pairs.  The results can been seen in the two tables below.

```{r}
#Displaying highly correlated pairs
cor.plt <- cor(cstData, use = "pairwise.complete.obs", method = "pearson")
cor.plt[lower.tri(cor.plt,diag=TRUE)] = NA #Prepare to drop duplicates and meaningless information
cor.plt <- as.data.frame(as.table(cor.plt)) #Turn into a 3-column table
cor.plt <- na.omit(cor.plt) #Get rid of the junk (NA's) we flagged above
cor.plt <- subset(cor.plt, abs(cor.plt$Freq)>0.85)
cor.plt <- cor.plt[order(-abs(cor.plt$Freq)),] #Sort by highest correlation (whether +ve or -ve)
rownames(cor.plt) <- c()
names(cor.plt)[3] <- "Correlation"
kab_tab(cor.plt, cap="Highly Correlated Variable Pairs")
```

```{r}
vars <- c(as.character(cor.plt$Var1), as.character(cor.plt$Var2))
vars <- as.data.frame(table(vars))
vars <- vars[order(-vars$Freq),]
rownames(vars) <- c()
kab_tab(vars, cap="Frequency of Variables Involved in Highly Correlated Pairs")
```

What we found is that there were exactly 5 variables that were most frequently associated with highly correlated pairs.  Each of these variables, `Alch.Rel`, `Balling`, `Balling.Lvl`, `Carb.Rel`, `Density` was involved in 4 pairs.  None of the other variables were involved in more than one pair.  By removing `Balling.Lvl`, we could eliminate 4 of our highly correlated pairs, by removing `Density` we got rid of 3 more, and with `Balling` 2 more. so we were left with only the following 4 pairs of highly correlated variables.  

```{r}
cstData <- subset(cstData, select = -c(Balling.Lvl, Density, Balling))
cor.plt <- cor(cstData, use = "pairwise.complete.obs", method = "pearson")
cor.plt[lower.tri(cor.plt,diag=TRUE)] = NA #Prepare to drop duplicates and meaningless information
cor.plt <- as.data.frame(as.table(cor.plt)) #Turn into a 3-column table
cor.plt <- na.omit(cor.plt) #Get rid of the junk (NA's) we flagged above
cor.plt <- subset(cor.plt, abs(cor.plt$Freq)>0.85)
cor.plt <- cor.plt[order(-abs(cor.plt$Freq)),] #Sort by highest correlation (whether +ve or -ve)
rownames(cor.plt) <- c()
names(cor.plt)[3] <- "Correlation"
kab_tab(cor.plt, cap="Highly Correlated Variable Pairs")
```

Each of the variables in these pairs only appears once so we needed to get rid of one variable from each pair in order to eliminate all pairs of variables with a 0.85 correlation or more.  So we decide to remove the 4 with the lowest correlation to `PH` without removing two from the same pair.  This eliminated `Filler.Speed`, `Alch.Rel`, `Hyd.Pressure2` and `Filler.Level`.  

```{r}
temp <- subset(cstData, 
               select = c(Alch.Rel, Bowl.Setpoint, Carb.Rel,	Filler.Level,
                          Filler.Speed, Hyd.Pressure2, Hyd.Pressure3,	MFR, PH))
temp <- (cor(temp, use = "pairwise.complete.obs", method = "pearson")[1:8,9])
sort(abs(temp))
```

```{r}
cstData <- subset(cstData, select = -c(Filler.Speed, Alch.Rel, Hyd.Pressure2, Filler.Level))
```

So in the end we still removed 7 variables but not all the same ones recommended by the `findCorrelation` function.  Only 2 of the variables recommended by the function matched our list, `Hyd.Pressure2` and  `Filler.Level`.

# Models

Next we partitioned our dataset into training and validation subsets by randomly selecting 70% for training and leaving the remaining 30% set aside for testing.

```{r}
#Partition Data
set.seed(123)
trainidx<-sample(nrow(cstData),round(0.7*nrow(cstData)),replace=F)
traindata<-cstData[trainidx,]
testdata<-cstData[-trainidx,]
```

We then tuned a full range of model types including: Linear Regression, Ridge Regression, Lasso, Random Forest, Tree Bag, CTree, Classification and Regression Tree (CART), Multivariate Adaptive Regression Splines (MARS), K-Nearest Neighbors (KNN) and Support Vector Machine (SVM) using repeated cross-validation on all models.  The RMSE, $R^2$, and MAE statistics for each of these models are presented in the table below, ordered by the lowest RMSE to highest and thus best predictive performance to worst.

```{r cache=TRUE}
require(caret)
set.seed(123)
trctrl<- trainControl(method="repeatedcv", number=3, repeats=2)

##Linear Regression
linreg <- caret::train(PH~., data=traindata, method="lm", 
                trControl=trctrl)
linPred <- predict(linreg, newdata = testdata)
m1<-data.frame(postResample(pred = linPred, obs = testdata$PH)) #0.1414880 0.3775156

##Ridge Regression
ridge <- caret::train(PH~., data=traindata, method="ridge", 
                trControl=trctrl)
ridgePred <- predict(ridge, newdata = testdata)
m2<-data.frame(postResample(pred = ridgePred, obs = testdata$PH)) #0.1414837 0.3775762

##Lasso Regression
lasso <- caret::train(PH~., data=traindata, method="lasso", 
                trControl=trctrl)
lassoPred <- predict(lasso, newdata = testdata)
m3<-data.frame(postResample(pred = lassoPred, obs = testdata$PH)) #0.1418941 0.3762947

##RandomForest (Processed)
rforest <- caret::train(PH~., data=traindata, method="cforest", 
                trControl=trctrl,
                tuneLength =2)
forPred <- predict(rforest, newdata = testdata)
m4<-data.frame(postResample(pred = forPred, obs = testdata$PH)) #0.11550844 0.59451507

##Tree Bag
bag <- caret::train(PH~., data=traindata, method="treebag", 
                trControl=trctrl,
                tuneLength =2)
bagPred <- predict(bag, newdata = testdata)
m5<-data.frame(postResample(pred = bagPred, obs = testdata$PH)) #0.12790436 0.50718903

##CTree
ctre <- caret::train(PH~., data=traindata, method="ctree2", 
                trControl=trctrl,
                tuneLength =2)
ctrePred <- predict(ctre, newdata = testdata)
m6<-data.frame(postResample(pred = ctrePred, obs = testdata$PH)) #0.1519582 0.2804008

##CART
rcart<- caret::train(PH~., data=traindata, method="rpart",
                trControl=trctrl,
                tuneLength =2)
cartPred <- predict(rcart, newdata = testdata)
m7<-data.frame(postResample(pred = cartPred, obs = testdata$PH)) #0.1593219 0.2053639

##MARS
marsFit <- earth(PH~., data = traindata, degree=2, nprune=14)
marsPred <- predict(marsFit, newdata = testdata)
m8<-data.frame(postResample(pred = marsPred, obs = testdata$PH)) #0.1399919 0.3912855

##KNN
knnGrid <-  expand.grid(k = 1:20)
knnFit <- caret::train(PH~., data = traindata, 
                method = "knn",
                trControl = trctrl, 
                tuneGrid = knnGrid)
knnPred <- predict(knnFit, newdata = testdata)
m9<-data.frame(postResample(pred = knnPred, obs = testdata$PH)) #0.1278504 0.5088682

##SVM (Radial Kernel)
svmGrid <-  expand.grid(C = c(1,1000))
svmFit <- caret::train(PH~., data = traindata, 
                 #type='eps-regression',
                 method = 'svmRadialCost', 
                 trControl = trctrl, 
                 tuneGrid = svmGrid)
svmPred <- predict(svmFit, newdata = testdata)
m10<-data.frame(postResample(pred = svmPred, obs = testdata$PH)) #0.13136218 0.48751241

df<-data.frame(rbind(m1[,1],m2[,1],m3[,1],m4[,1],m5[,1],m6[,1],m7[,1],
                     m8[,1],m9[,1],m10[,1]))
rownames(df)<-c("Linear Regression","Ridge Regression","Lasso","Random Forest",
                "Tree Bag","CTree","CART","MARS","KNN","SVM")
colnames(df)<-c("RMSE","Rsquared","MAE")
df <- df[order(df$RMSE),]
kab_tab(df, cap="MODELS")
```

The random forest model was selected for our predictions based on the lowest RMSE and MAE statistics.  Although it also had the highest $R^2$ value that statistic should only be used to compare performance between variously tuned models of the same type, not between models of different types, so it's relevance is not significant in this case.  

```{r}
#Variable Importance Ranking (Random Forest)
rfImp <- varImp(rforest, scale = FALSE)
bookTheme()
plot(rfImp, top=15, scales = list(y = list(cex = 0.8)))
```

### Top Five Variables by Importance Score

```{r}
options(digits = 5)
rfImp2 <- rfImp$importance[order(-rfImp$importance$Overall), , drop=FALSE]
kab_tab2(head(rfImp2, 5), cap="Variable Importance Scores")
options(digits = 2)
```

```{r}
#install.packages('rpart.plot')
library(rpart.plot)
tree <- rpart(PH~., data=traindata)
prp(tree)
```


```{r}
library(randomForest)
#Using full dataset (applying na.roughfix to missing values)
cstData_all<-subset(stData[-2])
set.seed(123)
trainidx2<-sample(nrow(cstData_all),round(0.7*nrow(cstData_all)),replace=F)
traindata2<-cstData_all[trainidx2,]
testdata2<-cstData_all[-trainidx2,]

##Additional Random Forest tuning (TUNE HERE)
#rf.model2 <- randomForest(PH~., data=traindata2, na.action=na.roughfix)
rf.model2 <- randomForest(PH~., data=traindata2, na.action=na.roughfix, importance=TRUE) 
rfPred2 <- predict(rf.model2, newdata = testdata2)
postResample(pred = rfPred2, obs = testdata2$PH) #0.1097018 0.6173039
```

```{r}
#with importance=TRUE, uses approach by Breiman to calculate the variable importance reported as MeanDecreaseAccuracy
#https://stackoverflow.com/questions/37888619/difference-between-varimp-caret-and-importance-randomforest-for-random-fores
rfImp2 <- as.data.frame(importance(rf.model2, scale = FALSE))
rfImp2[order(-rfImp2[,2]),]
```

### Top Five Most Important 
Mnf.Flow	1.215420e-02	6.8574948		
Usage.cont	5.769698e-03	4.5754067		
Bowl.Setpoint	5.173735e-03	2.8063547		
Temperature	2.667381e-03	2.6427466		
Carb.Rel	3.768302e-03	2.4322423



```{r}
#Comparing Adj. Rsquared for OLS Linear Regression models, inputting the top-ten most important variables from Random Forest
##Linear Regression using all variables (Mnf.Flow_ord instead of Mnf.Flow)
linreg2 <- caret::train(PH~., data=traindata, method="lm",  trControl=trctrl)
summary(linreg2) #Adjusted R-squared:  0.356

##Linear Regression using only top 10 variables 
linreg3 <- caret::train(PH~Mnf.Flow+Usage.cont+Bowl.Setpoint+Pressure.Vacuum+Temperature+Oxygen.Filler+Hyd.Pressure3+Carb.Pressure1+Hyd.Pressure1, data=traindata, method="lm",  trControl=trctrl)
summary(linreg3) # Adjusted R-squared:  0.328 

##Linear Regression using only top 8 variables (Removing Pressure.Vacuum and Hyd.Pressure1)
linreg4 <- caret::train(PH~Mnf.Flow+Usage.cont+Bowl.Setpoint+Temperature+Oxygen.Filler+Hyd.Pressure3+Carb.Pressure1, data=traindata, method="lm", trControl=trctrl)
summary(linreg4) # Adjusted R-squared:  0.329  
```

__note the negative coefficients of Mnf.Flow, Usage.conf, Temperature; positive of Alch.Rel, Bowl.Setpoint, Hyd.Pressure3, Carb.Pressure1__


```{r}
#Visualing the Mnf.Flow column
plot(stData$Mnf.Flow, stData$PH)
hist(stData$Mnf.Flow)

```


```{r}
#Investigating percent of null values in ranged bins of Mnf.Flow 
nrow(stData) #2571 rows
sum(!complete.cases(stData)) #442 rows with missing data

neg.Mnf.Flow.count <- nrow(stData[stData$Mnf.Flow < 0,]) #1186 rows with negative values
neg.complete.Mnf.Flow.count <- sum(complete.cases(stData[stData$Mnf.Flow < 0,])) #989 complete cases
neg.complete.Mnf.Flow.count / neg.Mnf.Flow.count #0.8338954

nearzeros.Mnf.Flow.count <- nrow(stData[stData$Mnf.Flow >= 0 & stData$Mnf.Flow <= 1 ,]) #81 rows
nearzeros.complete.Mnf.Flow.count <-sum(complete.cases(stData[stData$Mnf.Flow >= 0 & stData$Mnf.Flow <= 1 ,]))
nearzeros.complete.Mnf.Flow.count / nearzeros.Mnf.Flow.count #0.01234568 1% of data complete

pos.Mnf.Flow.count <- nrow(stData[stData$Mnf.Flow > 1,]) #1308 total rows with with Mnf.Flow greater than 50
pos.complete.Mnf.Flow.count <- sum(complete.cases(stData[stData$Mnf.Flow > 1,])) #1140 complete cases with Mnf.Flow greater than 50
pos.complete.Mnf.Flow.count / pos.Mnf.Flow.count #0.8707951 87% of data complete


nearzeros.Mnf.Flow.count / nrow(stData) #.03150 3% of the total number of rows accounts for 18% of the missing values

```


```{r}
#Finding the mean of the Mnf.FLow from subset of values greater than 1, used in binning
#pos.Mnf.Flow <- stData[stData$Mnf.Flow > 1,]
#mean(pos.Mnf.Flow$Mnf.Flow, na.rm = T)
```


```{r}
#Separating the Mnf.Flow column by thresholds
stData$Mnf.Flow_ord <- cut(
  stData$Mnf.Flow,
  breaks = c(-Inf, -1, 1, 140, Inf),
  labels = c(1, 2, 3, 4),
  right  = FALSE
)
table(stData$Mnf.Flow_ord)
```

```{r}
#Violin plot of Mnf.Flow by bins
g <-ggplot(stData, aes(x=factor(stData$Mnf.Flow_ord), y=stData$PH))
g+geom_violin(alpha=0.5, color='grey') +
  geom_jitter(alpha=0.5, size=4, aes(), position = position_jitter(width = 0.1), color='darkblue', show.legend=FALSE) +
  ggtitle("PH by Mnf.Flow classification") +
  coord_flip()
```


```{r}
#Subsetting data by brand
brandA <- stDatao[stDatao$Brand.Code == 'A',]
brandB <- stDatao[stDatao$Brand.Code == 'B',]
brandC <- stDatao[stDatao$Brand.Code == 'C',]
brandD <- stDatao[stDatao$Brand.Code == 'D',]
```

```{r}

###Add Trimmed Means to NA Value
r <- colnames(cstData_all)[ apply(cstData_all, 2, anyNA)]

cstData_all[,colnames(cstData_all) %in% r]<-data.frame(sapply(cstData_all[,colnames(cstData_all) %in% r],
      function(x) ifelse(is.na(x),
            mean(x, na.rm = TRUE, trim = .1),
            x)))
```


```{r}
df<-data.frame()
#BrandA Training/Test Splitting
set.seed(123)
trainidxA<-sample(nrow(brandA),round(0.7*nrow(brandA)),replace=F)
traindataA<-cstData_all[trainidxA,]
testdataA<-cstData_all[-trainidxA,]

##RandomForest (on BrandA)
trctrl<- trainControl(method="repeatedcv", number=2,repeats=2)
rforestA <- caret::train(PH~., data=traindataA, method="cforest", 
                trControl=trctrl, tuneLength =2, na.action=na.omit)
forPredA <- predict(rforestA, newdata = testdataA)
l1<-data.frame(postResample(pred = forPredA, obs = testdataA$PH))

#BrandB Training/Test Splitting
set.seed(123)
trainidxB<-sample(nrow(brandB),round(0.7*nrow(brandB)),replace=F)
traindataB<-cstData_all[trainidxB,]
testdataB<-cstData_all[-trainidxB,]

##RandomForest (on BrandB)
trctrl<- trainControl(method="repeatedcv", number=2,repeats=2)
rforestB <- caret::train(PH~., data=traindataB, method="cforest", 
                trControl=trctrl, tuneLength =2, na.action=na.omit)
forPredB <- predict(rforestB, newdata = testdataB)
l2<-data.frame(postResample(pred = forPredB, obs = testdataB$PH))

#BrandC Training/Test Splitting
set.seed(123)
trainidxC<-sample(nrow(brandC),round(0.7*nrow(brandC)),replace=F)
traindataC<-cstData_all[trainidxC,]
testdataC<-cstData_all[-trainidxC,]

##RandomForest (on BrandC)
trctrl<- trainControl(method="repeatedcv", number=2,repeats=2)
rforestC <- caret::train(PH~., data=traindataC, method="cforest", 
                trControl=trctrl, tuneLength =2, na.action=na.omit)
forPredC <- predict(rforestC, newdata = testdataC)
l3<-data.frame(postResample(pred = forPredC, obs = testdataC$PH))

#BrandD Training/Test Splitting
set.seed(123)
trainidxD<-sample(nrow(brandD),round(0.7*nrow(brandD)),replace=F)
traindataD<-cstData_all[trainidxD,]
testdataD<-cstData_all[-trainidxD,]

##RandomForest (on BrandD)
trctrl<- trainControl(method="repeatedcv", number=2,repeats=2)
rforestD <- caret::train(PH~., data=traindataD, method="cforest", 
                trControl=trctrl, tuneLength =2, na.action=na.omit)
forPredD <- predict(rforestD, newdata = testdataD)
l4<-data.frame(postResample(pred = forPredD, obs = testdataD$PH))

kab_tab(table(stDatao$Brand.Code), cap="Brand Frequency")

df<-data.frame()
df<-rbind(l1[,1],l2[,1],l3[,1],l4[,1])
rownames(df)<-c("A","B","C","D")
colnames(df)<-c("RMSE","Rsquared","MAE")
kab_tab(df, cap="BRANDS")
```


```{r}

#Variable Importance Ranking (on Brand A)
rfImpA <- varImp(rforestA, scale = FALSE)
plot(rfImpA, top=5, scales = list(y = list(cex = 0.8)))

#Variable Importance Ranking (on Brand B)
rfImpB <- varImp(rforestB, scale = FALSE)
plot(rfImpB, top=5, scales = list(y = list(cex = 0.8)))

#Variable Importance Ranking (on Brand C)
rfImpC <- varImp(rforestC, scale = FALSE)
plot(rfImpC, top=5, scales = list(y = list(cex = 0.8)))

#Variable Importance Ranking (on Brand D)
rfImpD <- varImp(rforestD, scale = FALSE)
plot(rfImpD, top=5, scales = list(y = list(cex = 0.8)))


```

**Plot Goodness of fit**

We see that our predictors fall close to the line.

```{r}

eval_p<-predict(rf.model2,stEval, type = "response")
summary(eval_p)


PHMut <- mutate(stEval, predProb = predict(rf.model2 ,stEval,type = "response"))
grpPH <- group_by(PHMut, cut(eval_p, breaks = unique(quantile(eval_p, (0:25)/26, na.rm=TRUE))))

#hosmer-lemeshow stat
hlDf <- summarise(grpPH, y= sum(PH), pPred=mean(predProb), count = n())
hlDf <- mutate(hlDf, se.fit=sqrt(pPred * (1-(pPred)/count)))
ggplot(hlDf,aes(x=pPred,y=y/count,ymin=y/count-2*se.fit,ymax=y/count+2*se.fit)) +
    geom_point()+geom_linerange(color=grey(0.75))+geom_abline(intercept=0,slope=1) +
    xlab("Predicted Probability") +
    ylab("Observed Proportion")

write.csv(eval_p,"predicted_eval_values_PH.csv")
```


# Appendix

**Code used in analysis**
```{r, ref.label=knitr::all_labels(),echo=TRUE,eval=FALSE}

```

